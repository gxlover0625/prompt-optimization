# prompt-optimization



## Update
0714, agent base & chat function
```python
from common import backend

cfg = {
    "backend": "ollama",
    "model": "qwen2:7b",
    "api_key": "sk-proj-1234567890",
    "base_url": "http://localhost:11434/v1",
}

llm = backend[cfg['backend']](cfg)
print(llm.chat("ä½ å¥½"))
```
- coreæ˜¯æŠ½è±¡ç±»ï¼Œä¸»è¦æœ‰LLMåŸºç±»å’Œå®ƒçš„chatæ¥å£ã€AgentåŸºç±»å’Œå®ƒçš„runæ¥å£ã€ExecutionAgentåŸºç±»å’Œå®ƒçš„executeæ¥å£
- commonæ˜¯å…·ä½“å®ç°ï¼ŒåŒ…æ‹¬LLMåŸºç±»çš„ä¸‰ä¸ªå…·ä½“å®ç°ï¼Œollamaã€openaiã€vllm
- datasetå¤„ç†æ•°æ®
- piplineå¯¹åº”å®Œæ•´æ–¹æ³•

## ğŸ“Š Data Preparation
Please refer to the [docs/prepare_dataset.md](docs/prepare_dataset.md).

## ğŸ“– Model Preparation
Please refer to the [docs/prepare_model.md](docs/prepare_model.md).

## ğŸ—ï¸ ï¸QuickStart
Before evaluation, you need to read the [Data Preparation](#-data-preparation) and [Model Preparation](#-model-preparation) first.
```bash
git clone https://github.com/gxlover0625/prompt-optimization.git
cd prompt-optimization # please make sure you are in the root directory of the project

# direct run Qwen3-14B on Liar dataset
python src/main.py --pipline direct --model qwen3-14b_vllm --dataset liar --output_dir ./output
```
After running, you will get the results in the `output/direct_Qwen3-14B_Liar_{timestamp}/results.json`

## ğŸ¤ Acknowledgements
We were inspired by the excellent open-source project [OpenCompass](https://github.com/open-compass/opencompass), which helped simplify our development. Additionally, we would like to thank the following open-source projects for their code contributions.
- [OpenCompass](https://github.com/open-compass/opencompass) ![Star](https://img.shields.io/github/stars/open-compass/opencompass.svg?style=social&label=Star), is an LLM evaluation platform, supporting a wide range of models (Llama3, Mistral, InternLM2,GPT-4,LLaMa2, Qwen,GLM, Claude, etc) over 100+ datasets.
- [LMOps](https://github.com/microsoft/LMOps) ![Star](https://img.shields.io/github/stars/microsoft/LMOps.svg?style=social&label=Star), general technology for enabling AI capabilities w/ LLMs and MLLMs